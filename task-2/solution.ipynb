{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install pyspark --quiet\n",
    "%pip install -U -q PyDrive --quiet\n",
    "%pip install numpy pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALA_VERSION = \"2.12\"\n",
    "KAFKA_VERSION = \"3.7.0\"\n",
    "PYSPARK_SCALA_VERSION = \"2.12\"\n",
    "SPARK_VERSION = \"3.5.1\"\n",
    "\n",
    "%env SCALA_VERSION=$SCALA_VERSION\n",
    "%env KAFKA_VERSION=$KAFKA_VERSION\n",
    "%env PYSPARK_SCALA_VERSION=$PYSPARK_SCALA_VERSION\n",
    "%env SPARK_VERSION=$SPARK_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash start-kafka.sh\n",
    "!tail -n 100 kafka/logs/server.log | grep -i \"Kafka Server started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC1 = \"topic1\"\n",
    "TOPIC2 = \"topic2\"\n",
    "BOOTSTRAP_SERVER = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash create-topics.sh {BOOTSTRAP_SERVER} {TOPIC1} {TOPIC2}\n",
    "!rm -rf generator.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = [\n",
    "    \"python3\", \"generator.py\",\n",
    "    \"--topic1\", TOPIC1,\n",
    "    \"--topic2\", TOPIC2,\n",
    "    \"--bootstrap_server\", BOOTSTRAP_SERVER\n",
    "]\n",
    "\n",
    "GENERATOR = subprocess.Popen(command)\n",
    "print(f\"GENERATOR PID: {GENERATOR.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{PYSPARK_SCALA_VERSION}:{SPARK_VERSION}',\n",
    "    f'org.apache.kafka:kafka-clients:{KAFKA_VERSION}'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .appName(\"PDD task-2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", f\"{TOPIC1},{TOPIC2}\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming.state import GroupState\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "SAMPLE_INTERVAL = 100_000\n",
    "SAMPLE_COUNT = 20\n",
    "SAMPLE_SIZE = 1_000\n",
    "LAMBDA = 1e-5\n",
    "BRS_INSERTION_PROBABILITY = SAMPLE_SIZE * LAMBDA\n",
    "\n",
    "\n",
    "DATA_SCHEMA = StructType([\n",
    "    StructField(\"time_point\", IntegerType(), True),\n",
    "    StructField(\"value\", FloatType(), True)\n",
    "])\n",
    "\n",
    "OUTPUT_SCHEMA = StructType([\n",
    "    StructField(\"RS_time_point\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"RS_value\", ArrayType(FloatType()), False),\n",
    "    StructField(\"BRS_time_point\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"BRS_value\", ArrayType(FloatType()), False),\n",
    "    StructField(\"topic\", StringType(), False)\n",
    "])\n",
    "\n",
    "STATE_SCHEMA = StructType([\n",
    "    StructField(\"RS_time_point\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"RS_value\", ArrayType(FloatType()), False),\n",
    "    StructField(\"BRS_time_point\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"BRS_value\", ArrayType(FloatType()), False),\n",
    "    StructField(\"next_yield\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "def updateState(_, pdf_iter, state: GroupState):\n",
    "    (rs_tp, rs_v, brs_tp, brs_v, next_yield_tp) = state.get if state.exists else ([], [], [], [], SAMPLE_INTERVAL)\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        pdf = pdf.sort_values(by=\"time_point\")\n",
    "        for _, row in pdf.iterrows():\n",
    "            row_tp = row[\"time_point\"]\n",
    "            row_v = row[\"value\"]\n",
    "            # RS\n",
    "            if len(rs_tp) < SAMPLE_SIZE:\n",
    "                rs_tp.append(row_tp)\n",
    "                rs_v.append(row_v)\n",
    "            else:\n",
    "                pos = random.randrange(row_tp)\n",
    "                if pos < SAMPLE_SIZE:\n",
    "                    rs_tp[pos] = row_tp\n",
    "                    rs_v[pos] = row_v\n",
    "            # BRS\n",
    "            if random.random() < BRS_INSERTION_PROBABILITY:\n",
    "                if random.random() < len(brs_tp) / SAMPLE_SIZE:\n",
    "                    pos = random.randrange(len(brs_tp))\n",
    "                    brs_tp[pos] = row_tp\n",
    "                    brs_v[pos] = row_v\n",
    "                else:\n",
    "                    brs_tp.append(row_tp)\n",
    "                    brs_v.append(row_v)\n",
    "            # YIELD\n",
    "            if row_tp > next_yield_tp:\n",
    "                print(f\"Topic {row['topic']}: passed {next_yield_tp} timepoint\", end=\"\\033[K\\n\", flush=True)\n",
    "                next_yield_tp += SAMPLE_INTERVAL\n",
    "                yield pd.DataFrame({\n",
    "                    \"RS_time_point\": [rs_tp.copy()],\n",
    "                    \"RS_value\": [rs_v.copy()],\n",
    "                    \"BRS_time_point\": [brs_tp.copy()],\n",
    "                    \"BRS_value\": [brs_v.copy()],\n",
    "                    \"topic\": row[\"topic\"]\n",
    "                })\n",
    "\n",
    "    state.update((rs_tp, rs_v, brs_tp, brs_v, next_yield_tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.streaming.state import GroupStateTimeout\n",
    "\n",
    "sample_dict = {\n",
    "    TOPIC1: pd.DataFrame(columns=OUTPUT_SCHEMA.fieldNames()),\n",
    "    TOPIC2: pd.DataFrame(columns=OUTPUT_SCHEMA.fieldNames())\n",
    "}\n",
    "\n",
    "def collect_samples(next_sample, _):\n",
    "    global sample_dict\n",
    "    next_sample_pdf = next_sample.toPandas()\n",
    "    for topic in [TOPIC1, TOPIC2]:\n",
    "        sample_dict[topic] = pd.concat(\n",
    "            [sample_dict[topic], next_sample_pdf[next_sample_pdf[\"topic\"] == topic]],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "processing = df \\\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"value\").cast(\"string\"), DATA_SCHEMA)) \\\n",
    "    .select(\n",
    "        F.col(\"data.time_point\").alias(\"time_point\"),\n",
    "        F.col(\"data.value\").alias(\"value\"),\n",
    "        F.col(\"topic\")\n",
    "    ) \\\n",
    "    .groupBy(\"topic\") \\\n",
    "    .applyInPandasWithState(\n",
    "        updateState,\n",
    "        outputStructType=OUTPUT_SCHEMA,\n",
    "        stateStructType=STATE_SCHEMA,\n",
    "        outputMode=\"append\",\n",
    "        timeoutConf=GroupStateTimeout.NoTimeout\n",
    "    ) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(collect_samples) \\\n",
    "    .start()\n",
    "\n",
    "# Run until enough samples are collected\n",
    "print(\"Collecting samples...\")\n",
    "while processing.isActive:\n",
    "    processing.awaitTermination(5)\n",
    "    if all(len(sample_pdf) >= SAMPLE_COUNT for sample_pdf in sample_dict.values()):\n",
    "        print(f\"{SAMPLE_COUNT} samples collected from each topic.\")\n",
    "        break\n",
    "\n",
    "try:\n",
    "    # Send stop() signal\n",
    "    print(\"Sending stop() signal to the processing...\")\n",
    "    processing.stop()\n",
    "\n",
    "    # Wait for shutdown\n",
    "    print(\"Waiting 60sec for the processing to shutdown...\")\n",
    "    processing.awaitTermination(60)\n",
    "\n",
    "# Shutdown is messy, to say the least...\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "DEPTH_TP = 4\n",
    "\n",
    "# Trim the samples to have exactly SAMPLE_COUNT\n",
    "for topic, sample_pdf in sample_dict.items():\n",
    "    sample_dict[topic] = sample_pdf.drop(columns=[\"topic\"]).head(SAMPLE_COUNT)\n",
    "\n",
    "print(\"Kolmogorov-Smirnow test for stream distribution **COMPARISON**\\n\")\n",
    "for now_tp in range(SAMPLE_COUNT):\n",
    "    print(f\"now time period: {now_tp * SAMPLE_INTERVAL} - {(now_tp + 1) * SAMPLE_INTERVAL}\")\n",
    "    print(f\" RS test result: {kstest(sample_dict[TOPIC1].loc[now_tp,  'RS_value'], sample_dict[TOPIC2].loc[now_tp,  'RS_value'])}\")\n",
    "    print(f\"BRS test result: {kstest(sample_dict[TOPIC1].loc[now_tp, 'BRS_value'], sample_dict[TOPIC2].loc[now_tp, 'BRS_value'])}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Kolmogorov-Smirnow test for stream distribution **CHANGE**\\n\")\n",
    "for now_tp in range(SAMPLE_COUNT):\n",
    "    for old_tp in reversed(range(max(0, now_tp - DEPTH_TP), now_tp)):\n",
    "        print(f\"old time period: {old_tp * SAMPLE_INTERVAL} - {(old_tp + 1) * SAMPLE_INTERVAL}\")\n",
    "        print(f\"now time period: {now_tp * SAMPLE_INTERVAL} - {(now_tp + 1) * SAMPLE_INTERVAL}\")\n",
    "        print(f\"Topic: {TOPIC1}\")\n",
    "        print(f\"     RS test result: {kstest(sample_dict[TOPIC1].loc[now_tp,  'RS_value'], sample_dict[TOPIC1].loc[old_tp,  'RS_value'])}\")\n",
    "        print(f\"    BRS test result: {kstest(sample_dict[TOPIC1].loc[now_tp, 'BRS_value'], sample_dict[TOPIC1].loc[old_tp, 'BRS_value'])}\")\n",
    "        print(f\"Topic: {TOPIC2}\")\n",
    "        print(f\"     RS test result: {kstest(sample_dict[TOPIC2].loc[now_tp,  'RS_value'], sample_dict[TOPIC2].loc[old_tp,  'RS_value'])}\")\n",
    "        print(f\"    BRS test result: {kstest(sample_dict[TOPIC2].loc[now_tp, 'BRS_value'], sample_dict[TOPIC2].loc[old_tp, 'BRS_value'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, sample_pdf in sample_dict.items():\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(sample_pdf.to_string(max_colwidth=50))\n",
    "    for id, row in sample_pdf.iterrows():\n",
    "        print(f\"Row {id}:\")\n",
    "        for col in sample_pdf.columns:\n",
    "            print(f\"    Column {col} length: {len(row[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR.terminate()\n",
    "spark.stop()\n",
    "!bash kafka/bin/kafka-server-stop.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
