{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as PySQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/17 00:06:39 WARN Utils: Your hostname, DELL-laptop-14-5401 resolves to a loopback address: 127.0.1.1; using 172.20.97.216 instead (on interface eth0)\n",
      "24/05/17 00:06:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/17 00:06:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "#   GLOBALS   #\n",
    "###############\n",
    "SHINGLE_SIZE = 5\n",
    "BAND_SIZE    = 20\n",
    "ROW_SIZE     = 5\n",
    "\n",
    "TEST = True\n",
    "PERF = False\n",
    "\n",
    "# Spark\n",
    "SPARK = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"mySession\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Shingles\n",
    "SHINGLE_BASE = ord(\"Z\") - ord(\"A\") + 1\n",
    "\n",
    "# Minhash\n",
    "HASH_MOD = 1_000_000_007\n",
    "PERMUTATION_COUNT = BAND_SIZE * ROW_SIZE\n",
    "RAND_MAX = (2 ** 32) - 1\n",
    "PERMUTATION_ARR_BROADCAST = SPARK.sparkContext.broadcast(\n",
    "    np.array([\n",
    "        (randint(1, RAND_MAX), randint(0, RAND_MAX))\n",
    "        for _ in range(PERMUTATION_COUNT)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Group definition file\n",
    "GROUP_DEFINITION_PATH   = \"data/group_definition.json\" if TEST else \"data/bruh.json\" if PERF else \"data/group_definition.json\"\n",
    "GROUP_DEFINITION_SCHEMA = StructType([\n",
    "    StructField(\"group\", StringType(), False),\n",
    "    StructField(\"protein_list\", ArrayType(StringType(), False), False)\n",
    "])\n",
    "\n",
    "# Fasta directory\n",
    "FASTA_PATH   = \"data/test_fasta\" if TEST else \"data/bruh_fasta\" if PERF else \"data/fasta\"\n",
    "FASTA_SCHEMA = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"value\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#   HELPER FUNCTIONS   #\n",
    "########################\n",
    "def printSparkDetails(spark):\n",
    "    print(\"Details of SparkContext:\")\n",
    "    print(f\"App Name : {spark.sparkContext.appName}\")\n",
    "    print(f\"Master : {spark.sparkContext.master}\")\n",
    "\n",
    "def loadDataFrameGroupDefinition():\n",
    "    with open(GROUP_DEFINITION_PATH) as group_definitions_file:\n",
    "        return SPARK.createDataFrame(\n",
    "            json.load(group_definitions_file).items(),\n",
    "            GROUP_DEFINITION_SCHEMA\n",
    "        )\n",
    "\n",
    "def loadDataFrameFasta():\n",
    "    return SPARK.read.schema(FASTA_SCHEMA).json(FASTA_PATH)\n",
    "\n",
    "def shingle_int(shingle):\n",
    "    return sum(\n",
    "        (ord(aminoacid) - ord(\"A\")) * (SHINGLE_BASE ** exp)\n",
    "        for exp, aminoacid in enumerate(shingle[::-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#   WORKLOAD   #\n",
    "################\n",
    "def getMinhashesOfBands(value):\n",
    "    shingle_int_arr = np.array([\n",
    "        shingle_int(value[i : i + SHINGLE_SIZE])\n",
    "        for i in range(len(value) - SHINGLE_SIZE + 1)\n",
    "    ])\n",
    "\n",
    "    signature_arr = np.array([\n",
    "        np.min((a * shingle_int_arr + b) % HASH_MOD)\n",
    "        for a, b in PERMUTATION_ARR_BROADCAST.value\n",
    "    ])\n",
    "\n",
    "    signature_batch_hash_arr = np.array([\n",
    "        hash(tuple(signature_arr[i : i + ROW_SIZE]))\n",
    "        for i in range(0, PERMUTATION_COUNT, ROW_SIZE)\n",
    "    ])\n",
    "\n",
    "    return enumerate(signature_batch_hash_arr.tolist())\n",
    "\n",
    "minhash_tuple_type = StructType([\n",
    "    StructField(\"minhash_id\", IntegerType(), False),\n",
    "    StructField(\"minhash_value\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "udf_get_minhashes_of_bands = PySQL.udf(getMinhashesOfBands, ArrayType(minhash_tuple_type,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of SparkContext:\n",
      "App Name : mySession\n",
      "Master : local[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#   DATA FRAMES   #\n",
    "###################\n",
    "printSparkDetails(SPARK)\n",
    "\n",
    "# Load DFs\n",
    "df_group_definition = loadDataFrameGroupDefinition()\n",
    "df_fasta = loadDataFrameFasta()\n",
    "\n",
    "# Process group definition data\n",
    "df_group_statistics = df_group_definition \\\n",
    "    .withColumn(\n",
    "        \"group_count\",\n",
    "        (PySQL.size(PySQL.col(\"protein_list\"))).cast(LongType())\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"group_pairs\",\n",
    "        (PySQL.col(\"group_count\") * (PySQL.col(\"group_count\") - 1) / 2).cast(LongType())\n",
    "    ) \\\n",
    "    .select(\"group\", \"group_count\", \"group_pairs\")\n",
    "\n",
    "protein_count_total = df_group_statistics \\\n",
    "    .agg(PySQL.sum(\"group_count\").alias(\"protein_count_sum\")) \\\n",
    "    .collect()[0][\"protein_count_sum\"]\n",
    "\n",
    "df_group_statistics = df_group_statistics \\\n",
    "    .withColumn(\n",
    "        \"mixed_pairs\",\n",
    "        PySQL.col(\"group_count\") * (protein_count_total - PySQL.col(\"group_count\"))\n",
    "    ) \\\n",
    "    .select(\"group\", \"group_count\", \"group_pairs\", \"mixed_pairs\")\n",
    "\n",
    "# df_group_statistics.printSchema()\n",
    "# df_group_statistics.show(100, False)\n",
    "\n",
    "df_proteins = df_group_definition \\\n",
    "    .select(\"group\", PySQL.explode(\"protein_list\").alias(\"protein\"))\n",
    "\n",
    "# df_proteins.printSchema()\n",
    "# df_proteins.show(100, False)\n",
    "\n",
    "# LSH DF\n",
    "df_lsh = df_fasta \\\n",
    "    .withColumn(\"minhash_band_signature_list\", udf_get_minhashes_of_bands(\"value\")) \\\n",
    "    .select(\"name\", \"minhash_band_signature_list\") \\\n",
    "    .join(df_proteins, df_fasta.name == df_proteins.protein, \"left\") \\\n",
    "    .select(\"group\", \"name\", PySQL.explode(\"minhash_band_signature_list\").alias(\"minhash\"))\n",
    "\n",
    "# df_lsh.printSchema()\n",
    "# df_lsh.show(100, False)\n",
    "\n",
    "# Similarity DF\n",
    "df_similarity = df_lsh.alias(\"df_1\") \\\n",
    "    .join(\n",
    "        df_lsh.alias(\"df_2\"),\n",
    "        (PySQL.col(\"df_1.minhash\") == PySQL.col(\"df_2.minhash\")) & \\\n",
    "            (PySQL.col(\"df_1.name\") < PySQL.col(\"df_2.name\")),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        PySQL.col(\"df_1.group\").alias(\"group_1\"),\n",
    "        PySQL.col(\"df_2.group\").alias(\"group_2\"),\n",
    "        PySQL.col(\"df_1.name\").alias(\"name_1\"),\n",
    "        PySQL.col(\"df_2.name\").alias(\"name_2\"),\n",
    "        PySQL.col(\"df_1.minhash\").alias(\"minhash\")\n",
    "    ) \\\n",
    "    .dropDuplicates([\"group_1\", \"group_2\", \"name_1\", \"name_2\"])\n",
    "\n",
    "# df_similarity.printSchema()\n",
    "# df_similarity.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of true positive pairs:  13337\n",
      "No. of false positive pairs: 639\n",
      "No. of single group pairs:   31616541\n",
      "No. of mixed group pairs:    283677175\n",
      "True positive rate:  0.000421836152158454\n",
      "False positive rate: 2.252560502973142e-06\n",
      "Precision:           0.9946884686252626\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   STATISTICS   #\n",
    "##################\n",
    "true_positive_total = df_similarity \\\n",
    "    .filter(PySQL.col(\"group_1\") == PySQL.col(\"group_2\")) \\\n",
    "    .count()\n",
    "\n",
    "false_positive_total = df_similarity \\\n",
    "    .filter(PySQL.col(\"group_1\") != PySQL.col(\"group_2\")) \\\n",
    "    .count()\n",
    "\n",
    "single_group_pairs_total = df_group_statistics \\\n",
    "    .agg(PySQL.sum(\"group_pairs\").alias(\"group_pairs_sum\")) \\\n",
    "    .collect()[0][\"group_pairs_sum\"]\n",
    "\n",
    "mixed_group_pairs_total = df_group_statistics \\\n",
    "    .agg(PySQL.sum(\"mixed_pairs\").alias(\"mixed_pairs_sum\")) \\\n",
    "    .collect()[0][\"mixed_pairs_sum\"] \\\n",
    "    // 2\n",
    "\n",
    "print(f\"No. of true positive pairs:  {true_positive_total}\")\n",
    "print(f\"No. of false positive pairs: {false_positive_total}\")\n",
    "print(f\"No. of single group pairs:   {single_group_pairs_total}\")\n",
    "print(f\"No. of mixed group pairs:    {mixed_group_pairs_total}\")\n",
    "\n",
    "true_positive_rate  = true_positive_total  / single_group_pairs_total\n",
    "false_positive_rate = false_positive_total / mixed_group_pairs_total\n",
    "precision = true_positive_rate / (true_positive_rate + false_positive_rate)\n",
    "\n",
    "print(f\"True positive rate:  {true_positive_rate}\")\n",
    "print(f\"False positive rate: {false_positive_rate}\")\n",
    "print(f\"Precision:           {precision}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
